{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701f380a-3508-40c7-97d8-60c3d54b4f05",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: NPL on Intermittent Fasting and Keto Diet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0478b0-a8f6-4052-bb72-28d2f96e6e52",
   "metadata": {},
   "source": [
    "# Part 3: Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830babb7-b153-4e01-aa3e-b3028668070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0923955d-6fb3-405e-821a-4ced2def7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dealing with large data sets, so setting max number of column and row displays to be unlimited\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868b23e8-5d25-43d2-a7f0-fd29849bf3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the cleaned data\n",
    "df = pd.read_csv('data/subreddit_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ad77fe-b8e8-48fc-9101-5e8f119e35cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plateau sruggles</td>\n",
       "      <td>I (27F) have been intermittent fasting for abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625589667</td>\n",
       "      <td>123</td>\n",
       "      <td>660</td>\n",
       "      <td>27f intermittent fasting 2 months starting wei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I still do IF/OMAD now that I started exer...</td>\n",
       "      <td>I started lifting 4x/week (about 40 minutes), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625586042</td>\n",
       "      <td>127</td>\n",
       "      <td>679</td>\n",
       "      <td>started lifting 4xweek 40 minutes well taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new mindset</td>\n",
       "      <td>Hello everyone,\\n\\n  I am a mostly lurker here...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625584307</td>\n",
       "      <td>182</td>\n",
       "      <td>929</td>\n",
       "      <td>hello everyone mostly lurker reddit first id l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekend habits are making it difficult to loos...</td>\n",
       "      <td>Hi everyone,\\n\\nI have been doing IF (16:8) fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582039</td>\n",
       "      <td>110</td>\n",
       "      <td>569</td>\n",
       "      <td>hi everyone 168 almost 3 years remained consis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are these times acceptable for IF?</td>\n",
       "      <td>So, due to loss of employment, family has take...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582007</td>\n",
       "      <td>167</td>\n",
       "      <td>806</td>\n",
       "      <td>due loss employment family taken bother trying...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                   Plateau sruggles   \n",
       "1  Can I still do IF/OMAD now that I started exer...   \n",
       "2                                      A new mindset   \n",
       "3  Weekend habits are making it difficult to loos...   \n",
       "4                 Are these times acceptable for IF?   \n",
       "\n",
       "                                            selftext  subreddit  created_utc  \\\n",
       "0  I (27F) have been intermittent fasting for abo...          0   1625589667   \n",
       "1  I started lifting 4x/week (about 40 minutes), ...          0   1625586042   \n",
       "2  Hello everyone,\\n\\n  I am a mostly lurker here...          0   1625584307   \n",
       "3  Hi everyone,\\n\\nI have been doing IF (16:8) fo...          0   1625582039   \n",
       "4  So, due to loss of employment, family has take...          0   1625582007   \n",
       "\n",
       "   post_word_count  post_length  \\\n",
       "0              123          660   \n",
       "1              127          679   \n",
       "2              182          929   \n",
       "3              110          569   \n",
       "4              167          806   \n",
       "\n",
       "                                          clean_text  \n",
       "0  27f intermittent fasting 2 months starting wei...  \n",
       "1  started lifting 4xweek 40 minutes well taking ...  \n",
       "2  hello everyone mostly lurker reddit first id l...  \n",
       "3  hi everyone 168 almost 3 years remained consis...  \n",
       "4  due loss employment family taken bother trying...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c367d16a-7fc2-4111-9147-6d9149c3f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatizing\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    # split into words\n",
    "    split_text = text.split()\n",
    "\n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # lemmatize and rejoin\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a1c1f5-388e-4768-9c68-751ee6c891a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleantext_lemm'] = df['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e868a3-7417-4f44-bb56-40fea778cdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_length</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cleantext_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plateau sruggles</td>\n",
       "      <td>I (27F) have been intermittent fasting for abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625589667</td>\n",
       "      <td>123</td>\n",
       "      <td>660</td>\n",
       "      <td>27f intermittent fasting 2 months starting wei...</td>\n",
       "      <td>27f intermittent fasting 2 month starting weig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I still do IF/OMAD now that I started exer...</td>\n",
       "      <td>I started lifting 4x/week (about 40 minutes), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625586042</td>\n",
       "      <td>127</td>\n",
       "      <td>679</td>\n",
       "      <td>started lifting 4xweek 40 minutes well taking ...</td>\n",
       "      <td>started lifting 4xweek 40 minute well taking l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new mindset</td>\n",
       "      <td>Hello everyone,\\n\\n  I am a mostly lurker here...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625584307</td>\n",
       "      <td>182</td>\n",
       "      <td>929</td>\n",
       "      <td>hello everyone mostly lurker reddit first id l...</td>\n",
       "      <td>hello everyone mostly lurker reddit first id l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekend habits are making it difficult to loos...</td>\n",
       "      <td>Hi everyone,\\n\\nI have been doing IF (16:8) fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582039</td>\n",
       "      <td>110</td>\n",
       "      <td>569</td>\n",
       "      <td>hi everyone 168 almost 3 years remained consis...</td>\n",
       "      <td>hi everyone 168 almost 3 year remained consist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are these times acceptable for IF?</td>\n",
       "      <td>So, due to loss of employment, family has take...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582007</td>\n",
       "      <td>167</td>\n",
       "      <td>806</td>\n",
       "      <td>due loss employment family taken bother trying...</td>\n",
       "      <td>due loss employment family taken bother trying...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                   Plateau sruggles   \n",
       "1  Can I still do IF/OMAD now that I started exer...   \n",
       "2                                      A new mindset   \n",
       "3  Weekend habits are making it difficult to loos...   \n",
       "4                 Are these times acceptable for IF?   \n",
       "\n",
       "                                            selftext  subreddit  created_utc  \\\n",
       "0  I (27F) have been intermittent fasting for abo...          0   1625589667   \n",
       "1  I started lifting 4x/week (about 40 minutes), ...          0   1625586042   \n",
       "2  Hello everyone,\\n\\n  I am a mostly lurker here...          0   1625584307   \n",
       "3  Hi everyone,\\n\\nI have been doing IF (16:8) fo...          0   1625582039   \n",
       "4  So, due to loss of employment, family has take...          0   1625582007   \n",
       "\n",
       "   post_word_count  post_length  \\\n",
       "0              123          660   \n",
       "1              127          679   \n",
       "2              182          929   \n",
       "3              110          569   \n",
       "4              167          806   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  27f intermittent fasting 2 months starting wei...   \n",
       "1  started lifting 4xweek 40 minutes well taking ...   \n",
       "2  hello everyone mostly lurker reddit first id l...   \n",
       "3  hi everyone 168 almost 3 years remained consis...   \n",
       "4  due loss employment family taken bother trying...   \n",
       "\n",
       "                                      cleantext_lemm  \n",
       "0  27f intermittent fasting 2 month starting weig...  \n",
       "1  started lifting 4xweek 40 minute well taking l...  \n",
       "2  hello everyone mostly lurker reddit first id l...  \n",
       "3  hi everyone 168 almost 3 year remained consist...  \n",
       "4  due loss employment family taken bother trying...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ee527-347a-477f-bab4-4ab61b007dd3",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44701b85-2ad8-490d-b6ec-6c0c5cec9bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.544412\n",
       "0    0.455588\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f0bf-d13e-405c-a2a7-7a55f610d6fd",
   "metadata": {},
   "source": [
    "Given that we have quite balanced data between both classes, our baseline model accuracy is the probability from our target subreddit -- Keto diet. Our baseline accuracy is 54.4%. Hopefully we can models that score better than this score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a919348-feab-4825-b485-274c95c132fd",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6938e14-200e-421b-ac20-ce1a6e2ceb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleantext_lemm']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406bc50b-ee2f-462c-af68-991f060987a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7717,)\n",
      "(2573,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533de4ab-f12a-469d-afb1-fd8433f07f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer parameters:\n",
    "\n",
    "cvec_params = {\n",
    "    'cvec__max_features': [None, 5_000],\n",
    "    'cvec__max_df': [0.5, 0.9],\n",
    "    'cvec__ngram_range':[(1,1), (1,2)],\n",
    "}\n",
    "\n",
    "tvec_params = {\n",
    "    'tvec__max_features': [None, 5_000],\n",
    "    'tvec__max_df': [0.5, 0.9],\n",
    "    'tvec__ngram_range':[(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cdc5be7-f3ce-4e97-a0ed-b19a73255316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters:\n",
    "\n",
    "lr_params = {\n",
    "    'lr__penalty':['l1','l2'],\n",
    "    'lr__C':[0.1, 1, 10]\n",
    "}\n",
    "\n",
    "knn_params = {'knn__n_neighbors': [3, 5, 7],\n",
    "              'knn__weights': ['uniform', 'distance']}\n",
    "\n",
    "nb_params = {\n",
    "    'nb__alpha': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [100, 150],\n",
    "    'rf__max_depth': [None, 5],\n",
    "    'rf__min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "ada_params = {\n",
    "    'ada__n_estimators': [50, 100],\n",
    "    'ada__learning_rate': [0.5, 1.0]\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "    'gb__max_depth': [3, 4],\n",
    "    'gb__n_estimators': [100, 200],\n",
    "    'gb__learning_rate': [0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e0f6a1-a16f-402e-b6b2-2a3e88cf1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(vec_inst, vectorizer, mod_inst, model, vec_params, mod_params):\n",
    "    pipe = Pipeline([\n",
    "        (vec_inst, vectorizer),\n",
    "        (mod_inst, model)])\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid = {**vec_params, **mod_params})\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('*'*80)\n",
    "    print(f'MODEL = {model}, VECTORIZER = {vectorizer}')\n",
    "    print('*'*80)\n",
    "    print(f'Best fitting parameters: {gs.best_params_}\\n')\n",
    "    print(f'Best score: {round(gs.best_score_, 3)}')\n",
    "    print(f'Test score: {round(gs.score(X_test, y_test), 3)}\\n')\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = gs.predict(X_test)\n",
    "    \n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_test, preds))\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d');\n",
    "    \n",
    "    # Save best model results to Dataframe\n",
    "    df = pd.DataFrame()\n",
    "    df['model_vec'] = [f'{mod_inst}_{vec_inst}']\n",
    "    df['best_params'] = [gs.best_params_]\n",
    "    df['train_score'] = gs.best_score_\n",
    "    df['test_score'] = gs.score(X_test, y_test)\n",
    "    df['sensitivity'] =  tp / (tp + fn)\n",
    "    df['specificity'] = tn / (tn + fp)\n",
    "    df['precision'] = tp / (tp + fp)\n",
    "    df['f1_score'] = f1_score(y_test, preds)\n",
    "    df['tn'] = tn\n",
    "    df['fp'] = fp\n",
    "    df['fn'] = fn\n",
    "    df['tp'] = tp\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4587662d-4170-47bd-873d-0a62799bf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81174f4-a783-4630-9edd-ce06c915f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvec = model('cvec', CountVectorizer(), 'lr', LogisticRegression(), cvec_params, lr_params)\n",
    "lr_cvec.head().T\n",
    "\n",
    "df_all.append(lr_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8b3b5-dc30-47a6-8393-fef0b7a01c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tvec = model('tvec', TfidfVectorizer(), 'lr', LogisticRegression(), tvec_params, lr_params)\n",
    "lr_tvec.head().T\n",
    "\n",
    "df_all.append(lr_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27f9d0-44c0-473c-8381-fcde7866210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tvec = model('tvec', TfidfVectorizer(), 'knn', KNeighborsClassifier(), tvec_params, knn_params)\n",
    "knn_tvec.head().T\n",
    "\n",
    "df_all.append(knn_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e252934-b758-4928-b614-b71a95c3f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tvec = model('tvec', TfidfVectorizer(), 'knn', KNeighborsClassifier(), tvec_params, knn_params)\n",
    "knn_tvec.head().T\n",
    "\n",
    "df_all.append(knn_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1bac5-3c81-47e1-9dd3-4f180df77ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tvec = model('tvec', TfidfVectorizer(), 'nb', MultinomialNB(), tvec_params, nb_params)\n",
    "nb_tvec.head().T\n",
    "\n",
    "df_all.append(nb_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b39f0b-1ed9-4caf-8ec5-e4dc3b853b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tvec = model('tvec', TfidfVectorizer(), 'nb', MultinomialNB(), tvec_params, nb_params)\n",
    "nb_tvec.head().T\n",
    "\n",
    "df_all.append(nb_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2032f-9f2a-4392-bf32-28616aac175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tvec = model('tvec', TfidfVectorizer(), 'rf', RandomForestClassifier(), tvec_params, rf_params)\n",
    "rf_tvec.head().T\n",
    "\n",
    "df_all.append(rf_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a834c-9915-4fe5-8469-0db8cb02b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tvec = model('tvec', TfidfVectorizer(), 'rf', RandomForestClassifier(), tvec_params, rf_params)\n",
    "rf_tvec.head().T\n",
    "\n",
    "df_all.append(rf_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd76c61-8ef4-4e01-a33b-8fc2ca3a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_tvec = model('tvec', TfidfVectorizer(), 'ada', AdaBoostClassifier(), tvec_params, ada_params)\n",
    "ada_tvec.head().T\n",
    "\n",
    "df_all.append(ada_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5b3e8-d911-4a93-aa10-5bcd2c39367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_tvec = model('tvec', TfidfVectorizer(), 'ada', AdaBoostClassifier(), tvec_params, ada_params)\n",
    "ada_tvec.head().T\n",
    "\n",
    "df_all.append(ada_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ec29b-b89e-4e3d-a8ba-6b25d638f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_tvec = model('tvec', TfidfVectorizer(), 'gb', GradientBoostingClassifier(), tvec_params, gb_params)\n",
    "gb_tvec.head().T\n",
    "\n",
    "df_all.append(gb_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c307d6-0e45-459b-8c5c-02636b32cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_tvec = model('tvec', TfidfVectorizer(), 'gb', GradientBoostingClassifier(), tvec_params, gb_params)\n",
    "gb_tvec.head().T\n",
    "\n",
    "df_all.append(gb_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206eba8-4b68-4d85-9d94-6ffa3934025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71851d88-555e-4ba3-80ac-37212fb39dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "full_df.sort_values(by=['test_score'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f584f-de2d-4d7b-bf1c-fec675dced2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4c86a-6a25-4cc0-9d29-c1bf980f042c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a01dc-5007-44db-9fa9-0d859d3d8ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a88be8a-d2e4-404a-b612-5350386f82fb",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "My impression after reading fews posts that contain words of 'keto' and 'interminttent fast' is that many posters are there (on reddit) to seek for advices or suggestions to help on their process on interminttent fasting of keto. Few people share their sucess and health improvement during the process while some other people actually face health issue during the process of weight loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
