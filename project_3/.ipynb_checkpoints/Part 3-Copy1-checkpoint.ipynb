{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701f380a-3508-40c7-97d8-60c3d54b4f05",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: NPL on Intermittent Fasting and Keto Diet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0478b0-a8f6-4052-bb72-28d2f96e6e52",
   "metadata": {},
   "source": [
    "# Part 3: Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830babb7-b153-4e01-aa3e-b3028668070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0923955d-6fb3-405e-821a-4ced2def7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dealing with large data sets, so setting max number of column and row displays to be unlimited\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868b23e8-5d25-43d2-a7f0-fd29849bf3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the cleaned data\n",
    "df = pd.read_csv('data/subreddit_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ad77fe-b8e8-48fc-9101-5e8f119e35cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>post_length</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plateau sruggles</td>\n",
       "      <td>I (27F) have been intermittent fasting for abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625589667</td>\n",
       "      <td>123</td>\n",
       "      <td>660</td>\n",
       "      <td>27f intermittent fasting 2 months starting wei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I still do IF/OMAD now that I started exer...</td>\n",
       "      <td>I started lifting 4x/week (about 40 minutes), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625586042</td>\n",
       "      <td>127</td>\n",
       "      <td>679</td>\n",
       "      <td>started lifting 4xweek 40 minutes well taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A new mindset</td>\n",
       "      <td>Hello everyone,\\n\\n  I am a mostly lurker here...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625584307</td>\n",
       "      <td>182</td>\n",
       "      <td>929</td>\n",
       "      <td>hello everyone mostly lurker reddit first id l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekend habits are making it difficult to loos...</td>\n",
       "      <td>Hi everyone,\\n\\nI have been doing IF (16:8) fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582039</td>\n",
       "      <td>110</td>\n",
       "      <td>569</td>\n",
       "      <td>hi everyone 168 almost 3 years remained consis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are these times acceptable for IF?</td>\n",
       "      <td>So, due to loss of employment, family has take...</td>\n",
       "      <td>0</td>\n",
       "      <td>1625582007</td>\n",
       "      <td>167</td>\n",
       "      <td>806</td>\n",
       "      <td>due loss employment family taken bother trying...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                   Plateau sruggles   \n",
       "1  Can I still do IF/OMAD now that I started exer...   \n",
       "2                                      A new mindset   \n",
       "3  Weekend habits are making it difficult to loos...   \n",
       "4                 Are these times acceptable for IF?   \n",
       "\n",
       "                                            selftext  subreddit  created_utc  \\\n",
       "0  I (27F) have been intermittent fasting for abo...          0   1625589667   \n",
       "1  I started lifting 4x/week (about 40 minutes), ...          0   1625586042   \n",
       "2  Hello everyone,\\n\\n  I am a mostly lurker here...          0   1625584307   \n",
       "3  Hi everyone,\\n\\nI have been doing IF (16:8) fo...          0   1625582039   \n",
       "4  So, due to loss of employment, family has take...          0   1625582007   \n",
       "\n",
       "   post_word_count  post_length  \\\n",
       "0              123          660   \n",
       "1              127          679   \n",
       "2              182          929   \n",
       "3              110          569   \n",
       "4              167          806   \n",
       "\n",
       "                                          clean_text  \n",
       "0  27f intermittent fasting 2 months starting wei...  \n",
       "1  started lifting 4xweek 40 minutes well taking ...  \n",
       "2  hello everyone mostly lurker reddit first id l...  \n",
       "3  hi everyone 168 almost 3 years remained consis...  \n",
       "4  due loss employment family taken bother trying...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c367d16a-7fc2-4111-9147-6d9149c3f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatizing\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    # split into words\n",
    "    split_text = text.split()\n",
    "\n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # lemmatize and rejoin\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6f997a-3819-4ee2-b30f-1ecb5c5adec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "\n",
    "    # split into words\n",
    "    split_text = text.split()\n",
    "    \n",
    "    # instantiate stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # stem and rejoin\n",
    "    return ' '.join([stemmer.stem(word) for word in split_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ee527-347a-477f-bab4-4ab61b007dd3",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44701b85-2ad8-490d-b6ec-6c0c5cec9bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.544412\n",
       "0    0.455588\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f0bf-d13e-405c-a2a7-7a55f610d6fd",
   "metadata": {},
   "source": [
    "Given that we have quite balanced data between both classes, our baseline model accuracy is the probability from our target subreddit -- Keto diet. Our baseline accuracy is 54.4%. Hopefully we can models that score better than this score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a919348-feab-4825-b485-274c95c132fd",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6938e14-200e-421b-ac20-ce1a6e2ceb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406bc50b-ee2f-462c-af68-991f060987a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7717,)\n",
      "(2573,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea965f1-e037-43d0-a827-d21325049146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate a pipeline class with the following 3 as its list items:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()), # tuple for transformer object, class\n",
    "    ('nb', MultinomialNB()) # tuple for estimator object, class\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a53b538-9b36-4454-855a-b55268979552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params = {\n",
    "    'cvec__preprocessor' : [None, lemmatize_text, stem_text],\n",
    "    'cvec__max_features': [None, 5_000], # start with CountVectorizer() class' object cvec__CountVectorizer()'s hyperparameter\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)] # test unigram only (1,1) and unigram+bigram (1,2)\n",
    "} # standard param dict definition for GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32d89e5-c803-4386-b53a-a82a9099a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # the object that we are optimizing\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a28cb3-1038-4329-8980-ec9cfe93c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fcfa4-fdae-4a97-86c6-b7e3cc514d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b23a51-e56c-44a3-bd09-372909c53d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best paramter\n",
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc37b98-1297-46c5-b374-6d2d4d5acba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on training set.\n",
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fb649-568d-4cf4-8939-14595d98aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0ecc8-1039-4b12-b271-fb1de89e61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline([\n",
    "    ('tvec', CountVectorizer()), # tuple for transformer object, class\n",
    "    ('nb', MultinomialNB()) # tuple for estimator object, class\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81c402-13fb-4de3-acc4-3caa0648dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "pipe_params_2 = {\n",
    "    'tvec__preprocessor' : [None, lemmatize_title, stem_title],\n",
    "    'tvec__max_features': [None, 5_000],\n",
    "    'tvec__max_df': [0.5, 0.9],\n",
    "    'tvec__ngram_range':[(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a83ac-fd6b-455b-ae9b-1d06e90370b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs_2 = GridSearchCV(pipe_2, # the object that we are optimizing\n",
    "                  param_grid=pipe_params_2, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f317d89-9619-4530-a8b6-f1744e025347",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d75a2-6cd9-4621-b7cd-685ed0af1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34eed1-23c7-49fc-9738-510524f36504",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc23ea-0bb6-4f8e-8a36-39927688f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ce3ff-402a-41fb-9518-55a02c4d103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_2.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
