{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82166d95-45e8-4864-be84-49f55747efc8",
   "metadata": {},
   "source": [
    "## 02 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29025943-9ab4-4ef1-9b0b-5631d8bad965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "import numpy as np\n",
    "from pmaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782e0ba7-1e42-421e-a8dc-e42e4a0ef759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMAW is a wrapper for the Pushshift API which uses multithreading to retrieve Reddit comments and submissions. \n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd8951-e88e-4d6e-bfec-880cab0f97ea",
   "metadata": {},
   "source": [
    "Initial tests suggests that the two groups differ in the number of posts quite a lot, hence the traffic of the group with less posts (r/stocks) is considered when choosing the timeframe of the scraping process. As there are on average 4000 posts in r/Wallstreetbets and 1000 posts in r/Stocks every week, a one-month timeframme is taken to gurantee enough posts from r/stocks. The timeframe was choosen to be 2022 Jan 1st to 2022 Feb 1st. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1500fa-acc9-4ee8-a5cd-ed933c61b519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1643644800, 1640966400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the parameters for scraping. \n",
    "after=int(dt.datetime(2022,1,1,0,0).timestamp()) \n",
    "before=int(dt.datetime(2022,2,1,0,0).timestamp()) #2022 Jan 1st - 2022 Feb 1st.\n",
    "before,after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98ef25-bdb2-41ac-b769-36fc702ef476",
   "metadata": {},
   "source": [
    "Next, the search_submissions method will be used for scraping the posts, as it does not give disruption after certain number of requests. The documentation for search_submissions in PMAW library is at https://github.com/mattpodolak/pmaw/blob/master/README.md#pushshiftapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9211b5e-d82a-4da7-81c4-9f6caaaf80e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.81 s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submissions = api.search_submissions(subreddit=\"wallstreetbets\",after=after,before=before,safe_exit=True)\n",
    "df1 = pd.DataFrame(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb7ce8-0910-47ac-b0f1-5ddec3de5f1d",
   "metadata": {},
   "source": [
    "The wall time for scapping of 30-day data is below 2 min, this is acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41ce6e7-29e9-4817-842c-4498365a69aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 516 ms\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submission2=api.search_submissions(subreddit='stocks',after=after,before=before,safe_exit=True)\n",
    "df2=pd.DataFrame(submission2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574ff5cb-0d13-4d9c-8b5b-d012222bad28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>gallery_data</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>poll_data</th>\n",
       "      <th>live_audio</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>tournament_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>inkollusekar</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_1dt0l3xs</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Janto_2021</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Two Brains, Uses Neither'}]</td>\n",
       "      <td>Two Brains, Uses Neither</td>\n",
       "      <td>richtext</td>\n",
       "      <td>t2_88ab00y8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments        author author_flair_css_class  \\\n",
       "0            []                False  inkollusekar                   None   \n",
       "1            []                False    Janto_2021                   None   \n",
       "\n",
       "                              author_flair_richtext         author_flair_text  \\\n",
       "0                                                []                      None   \n",
       "1  [{'e': 'text', 't': 'Two Brains, Uses Neither'}]  Two Brains, Uses Neither   \n",
       "\n",
       "  author_flair_type author_fullname  author_is_blocked author_patreon_flair  \\\n",
       "0              text     t2_1dt0l3xs              False                False   \n",
       "1          richtext     t2_88ab00y8              False                False   \n",
       "\n",
       "   ... media_embed secure_media  secure_media_embed  gallery_data  \\\n",
       "0  ...         NaN          NaN                 NaN           NaN   \n",
       "1  ...         NaN          NaN                 NaN           NaN   \n",
       "\n",
       "   media_metadata author_cakeday poll_data live_audio banned_by  \\\n",
       "0             NaN            NaN       NaN        NaN       NaN   \n",
       "1             NaN            NaN       NaN        NaN       NaN   \n",
       "\n",
       "   tournament_data  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "\n",
       "[2 rows x 84 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b080289c-0a29-4c42-9728-c413fb71ba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22631, 84), (5085, 70))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape, df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bf501-aad6-4c8a-a0bf-53ccba4f158b",
   "metadata": {},
   "source": [
    "The number of posts in these two subreddits differ a lot, we will be working with an imbalanced classification problem later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d31d40-ba06-498f-b8d8-b9463a770b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.concat([df1,df2], axis=0)\n",
    "raw_data.to_csv('./data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466470a5-6c16-418a-a120-f5d23af8e82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
       "       'author_flair_type', 'author_fullname', 'author_is_blocked',\n",
       "       'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post',\n",
       "       'contest_mode', 'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_created_from_ads_ui', 'is_crosspostable', 'is_meta',\n",
       "       'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable',\n",
       "       'is_self', 'is_video', 'link_flair_background_color',\n",
       "       'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id',\n",
       "       'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked',\n",
       "       'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
       "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
       "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
       "       'subreddit_type', 'suggested_sort', 'thumbnail', 'title',\n",
       "       'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url',\n",
       "       'whitelist_status', 'wls', 'author_flair_background_color',\n",
       "       'author_flair_template_id', 'author_flair_text_color', 'post_hint',\n",
       "       'preview', 'thumbnail_height', 'thumbnail_width',\n",
       "       'url_overridden_by_dest', 'is_gallery', 'removed_by_category', 'media',\n",
       "       'media_embed', 'secure_media', 'secure_media_embed', 'gallery_data',\n",
       "       'media_metadata', 'author_cakeday', 'poll_data', 'live_audio',\n",
       "       'banned_by', 'tournament_data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48389a9b-e1cc-43cd-b533-fd3f8a5134e5",
   "metadata": {},
   "source": [
    "The titles of the posts are stored in the 'title' column, while the comments are stored in 'selftext' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "879f1ebc-fbb9-48a9-b861-9b781f25e1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check on the nan values of 'title' column\n",
    "df1.isna().sum()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a090ecb4-bd55-434f-af47-45c259f1a78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11298"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check on the nan values of 'selftext' column\n",
    "df1.isna().sum()['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ec127b8-05ee-45af-b9dc-b061d9469fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "163c734f-2377-4a69-b237-3e7dd092b121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2041"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9e86b-129b-4fef-963c-47be353deccd",
   "metadata": {},
   "source": [
    "The PMAW API has pre-filltered the data and all 'title' information are not empty. There are lots of nan values in 'selftext' column though. But since we will use the combination of 'title' and 'selftext' as the text corpus, those nan values of 'selftext' will not be nan values any more with 'title' text, hence we will keep these rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dd265-ee60-4edc-9e12-fd14608f1e62",
   "metadata": {},
   "source": [
    "Next we keep only the textual data from these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799cb568-b2ac-4987-814f-2642b713fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the title and comments and put into a 'text' column, and label the data\n",
    "df1['text']=df1['title']+df1['selftext']\n",
    "df1['label']='wallstreetbet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf540b65-e8e4-4c9a-88e6-fe446a2d84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['text']=df2['title']+df2['selftext']\n",
    "df2['label']='stock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e01d07-887d-4352-8a5a-c7edb22a33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the text and lable columns\n",
    "df1=df1[['label','text']]\n",
    "df2=df2[['label','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157cd93e-001b-40cb-af56-f7b1c89a5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of empty text in r/wallstreetbets: 0.06628076532190358%.\n",
      "percentage of empty text in r/stock: 0.2%.\n"
     ]
    }
   ],
   "source": [
    "print(f'percentage of empty text in r/wallstreetbets: {df1[\"text\"].isna().sum()/df1.shape[0]*100}%.')\n",
    "print(f'percentage of empty text in r/stock: {np.round(df2[\"text\"].isna().sum()/df2.shape[0]*100,2)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259eaea-1756-4d61-8732-55282365be9f",
   "metadata": {},
   "source": [
    "The percentage of null values in text is quite small, we can drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa07fd5-8468-419e-b564-87682d09acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the two dataframe together \n",
    "df_stack=pd.concat([df1,df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "841027ea-b19a-4f02-b533-be7aee5662b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54cd9e60-eb76-43a1-97e3-2034cb3647b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27691, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2255851a-99e1-4e26-a550-5e822fe94ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "df=df_stack\n",
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b28aec2e-d14c-4c50-a070-b31494110ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/textual_data_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d68750-c337-41f8-90c7-eb03ad57c1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
